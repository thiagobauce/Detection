# -*- coding: utf-8 -*-
"""DML_parte1_icmc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WA0JYxDvLErtdJ7-VtwdylzbIKVwgKtF
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim

from tqdm import tqdm
import numpy as np
import pdb
import time

torch.cuda.get_device_properties(0)

import os

!pwd



"""#Parameters"""

n_classes = 10
emb_size = 512
lr = 0.0005
batch_size = 512
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

"""# Prepare data loaders"""

!ls

transform = transforms.Compose([transforms.Resize((50,50)),
                                transforms.ToTensor(),
                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
                            ])

ds_train = torchvision.datasets.CIFAR10('./',train=True,transform=transform,download=True)
ds_test  = torchvision.datasets.CIFAR10('./',train=False,transform=transform,download=True)

!ls

n_classes = len(ds_train.classes)

dl_train = torch.utils.data.DataLoader(ds_train,batch_size=batch_size,drop_last=True)
dl_test  = torch.utils.data.DataLoader(ds_test,batch_size=batch_size,drop_last=True)

x,y = next(iter(dl_train))

x.shape,y.shape

y[0]

ds_train.classes

"""# Prepare model"""

model = torchvision.models.resnet50(pretrained=True)

model

import torchsummary

emb_size

model.fc = nn.Linear(model.fc.in_features,emb_size)

model.to(device)

"""##  Exerc√≠cio 1: modifique as redes """

model = torchvision.models.mobilenet_v3_small(pretrained=True)

model

model.classifier = nn.Linear(576,512)

model

x,y = next(iter(dl_train))

x.shape,y.shape


margin_loss = CombinedMarginLoss(
    64,
    1.0,
    0.5,
    0,
    0
)
loss_function = PartialFC_V2(margin_loss=margin_loss,embedding_size=emb_size, num_classes=n_classes, sample_rate=0.2)
loss_function.train().cuda()

"""# Training """

opt       = optim.AdamW(model.parameters(),lr=lr)
stop = False

model.to(device)

epoch = 0
patience = 10
start_time = time.perf_counter()
batch_loss = []
batch_loss_test = []
best_loss = 10000

start_time = time.perf_counter()
patience = 10
stop = False
while(not stop):
    batch_list_loss = []
    batch_iterator = tqdm(dl_train)
    for i,(x,y)  in enumerate(batch_iterator):
        x = x.to(device)
        y = y.to(device)
        embeddings = model(x)
        loss = loss_function(embeddings,y)

        opt.zero_grad()
        loss.backward()
        batch_list_loss.append(loss.item())
        opt.step()
    batch_loss.append(np.mean(batch_list_loss))
    print("training loss ",batch_loss[-1])

    with torch.no_grad():
        batch_list_loss = []
        for i,(x,y) in enumerate(dl_test):
            x = x.to(device)
            y = y.to(device)
            embeddings = model(x)
            loss = loss_function(embeddings,y)
            batch_list_loss.append(loss.item())
        batch_loss_test.append(np.mean(batch_list_loss))
    print("test loss ",batch_loss_test[-1])
    if batch_loss_test[-1] < best_loss:
        print("saving model")
        patience_wait = patience
        best_loss = batch_loss_test[-1]
        save_model = {'model':model.state_dict(),'opt':opt.state_dict(),'loss_training':batch_list_loss}
        torch.save(save_model,'best_model_v1.pth')
    patience_wait -= 1
    if patience_wait == 0:
        stop = True



    

    epoch +=1
end_time = time.perf_counter()

save_model = {'model':model.state_dict(),'opt':opt.state_dict(),'loss_training':batch_list_loss}

torch.save(save_model,'last_run.pth')

saved_model = torch.load('best_model_v1.pth')

model.load_state_dict(saved_model['model'])

"""# Evaluation"""

with torch.no_grad():
    batch_list_loss = []
    train_embeddings = []
    train_y          = []
    test_embeddings = []
    test_y          = []
    for i,(x,y) in enumerate(dl_train):
        x = x.to(device)
        y = y.to(device)
        embeddings = model(x)
        train_embeddings.append(embeddings)
        train_y.append(y)

    for i,(x,y) in enumerate(dl_test):
        x = x.to(device)
        y = y.to(device)
        embeddings = model(x)
        test_embeddings.append(embeddings)
        test_y.append(y)

jtrain_emb = torch.cat(train_embeddings,axis=0)
jtest_emb = torch.cat(test_embeddings,axis=0)

jtrain_emb.shape

xtrain = jtrain_emb.cpu().numpy()
xtest  = jtest_emb.cpu().numpy()

ytrain = torch.cat(train_y).cpu().numpy()
ytest  = torch.cat(test_y).cpu().numpy()

import sklearn.neighbors

knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1,weights='distance')

knn.fit(xtrain,ytrain)

pred = knn.predict(xtest)
#predp = knn.predict_proba(xtest)

import sklearn.metrics as metrics

print("precision ", metrics.precision_score(ytest,pred,average='macro'))
print("recall    ", metrics.recall_score(ytest,pred,average='macro'))
print("f1        ", metrics.f1_score(ytest,pred,average='macro'))

print(metrics.classification_report(ytest,pred))

print(metrics.top_k_accuracy_score(ytest,predp,k=5))

ds_train.classes

"""# Pytorch metric learning"""

!pip install pytorch-metric-learning
!pip install faiss-gpu

from pytorch_metric_learning import distances, losses, miners, reducers, testers
from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator

distance = distances.CosineSimilarity()
reducer = reducers.ThresholdReducer(low=0)
loss_function = losses.ArcFaceLoss(margin=.2, distance=distance,reducer=reducer,num_classes=n_classes ,embedding_size =emb_size )